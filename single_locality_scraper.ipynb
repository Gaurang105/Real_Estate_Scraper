{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class PropertyScraper:\n",
    "    def __init__(self, url):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(url)\n",
    "        self.all_data = pd.DataFrame()\n",
    "\n",
    "    def set_search_parameters(self, sro, local, year):\n",
    "        self.select_element_by_id(\"ctl00_ContentPlaceHolder1_ddl_sro_s\", sro)\n",
    "        self.select_element_by_id(\"ctl00_ContentPlaceHolder1_ddl_loc_s\", local)\n",
    "        self.select_element_by_id(\"ctl00_ContentPlaceHolder1_ddl_year_s\", year)\n",
    "\n",
    "    def select_element_by_id(self, element_id, value):\n",
    "        element = self.driver.find_element(By.ID, element_id)\n",
    "        element.send_keys(value)\n",
    "\n",
    "    def get_captcha_image(self):\n",
    "        captcha_img_element = self.driver.find_element(By.XPATH, \"//div[@class='btn btn-sm']//img\")\n",
    "        captcha_img_url = captcha_img_element.get_attribute(\"src\")\n",
    "        image = Image.open(BytesIO(urlopen(captcha_img_url).read()))\n",
    "        image.show()\n",
    "\n",
    "    def submit_captcha(self, captcha_text):\n",
    "        self.driver.find_element(By.ID, \"ctl00_ContentPlaceHolder1_txtcaptcha_s\").send_keys(captcha_text)\n",
    "        self.driver.find_element(By.ID, \"ctl00_ContentPlaceHolder1_btn_search_s\").click()\n",
    "\n",
    "    def scrape_current_page(self):\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        df = df.dropna(subset=['First Party'])\n",
    "        df = df.iloc[:-1]\n",
    "        df.drop([\"Reg.No\", \"Deed Type\"], axis=1, inplace=True)\n",
    "        df.rename(columns={\"Area\": \"Area (sq. feet)\"}, inplace=True)\n",
    "        df[\"Area (sq. feet)\"] = df[\"Area (sq. feet)\"].str.replace(\"Sq. Feet\", \"\").str.strip()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def wait_for_element(self, by, value, timeout=10):\n",
    "        try:\n",
    "            element_present = EC.presence_of_element_located((by, value))\n",
    "            WebDriverWait(self.driver, timeout).until(element_present)\n",
    "        except TimeoutException:\n",
    "            return None\n",
    "        return self.driver.find_element(by, value)\n",
    "\n",
    "    def click_next_page(self):\n",
    "        next_page = self.wait_for_element(By.ID, \"ctl00_ContentPlaceHolder1_gv_search_ctl13_Button2\")\n",
    "        if next_page and next_page.is_enabled() and next_page.is_displayed():\n",
    "            self.driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def scrape_all_data(self):\n",
    "        self.all_data = self.scrape_current_page()\n",
    "        while True:\n",
    "            try:\n",
    "                time.sleep(2)\n",
    "                if self.click_next_page():\n",
    "                    time.sleep(2)\n",
    "                    new_data = self.scrape_current_page()\n",
    "                    self.all_data = pd.concat([self.all_data, new_data], ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "        self.all_data = self.all_data.reset_index(drop=True)\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "def main():\n",
    "    url = \"https://esearch.delhigovt.nic.in/Complete_search.aspx\"\n",
    "    sro = \"Central -Asaf Ali (SR III)\"\n",
    "    local = \"Ajmal Khan Road\"\n",
    "    year = \"2021-2022\"\n",
    "\n",
    "    # Initialize the scraper\n",
    "    scraper = PropertyScraper(url)\n",
    "    \n",
    "    # Set search parameters\n",
    "    scraper.set_search_parameters(sro, local, year)\n",
    "\n",
    "    # Show captcha image\n",
    "    scraper.get_captcha_image()\n",
    "\n",
    "    # Get captcha input from the user\n",
    "    captcha_text = input(\"Please enter the captcha text: \")\n",
    "\n",
    "    # Submit the captcha\n",
    "    scraper.submit_captcha(captcha_text)\n",
    "\n",
    "    # Scrape all data\n",
    "    scraper.scrape_all_data()\n",
    "\n",
    "    # Save the scraped data to a Excel file\n",
    "    scraper.all_data.to_excel(\"property_data.xlsx\", index=False)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    scraper.close_driver()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
